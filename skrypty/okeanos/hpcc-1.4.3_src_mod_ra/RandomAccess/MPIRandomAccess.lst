%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S u m m a r y   R e p o r t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Compilation
-----------
File     : /lustre/tetyda/home/lgorski/okeanos_scripts/randomaccess/hpcc-1.4.3_src_mod_ra/hpl/lib/arch/build/../../../../RandomAccess/MPIRandomAccess.c
Compiled : 2016-03-19  13:20:08
Compiler : Version 8.4.5
Ftnlx    : Version 8413 (libcif 84006)
Target   : x86-64
Command  : driver.cc -h cpu=haswell -h static -D __CRAYXC -D __CRAY_HASWELL
           -D __CRAYXT_COMPUTE_LINUX_TARGET -h network=aries
           -o ../../../../RandomAccess/MPIRandomAccess.o
           -c ../../../../RandomAccess/MPIRandomAccess.c -I ../../../../include
           -I ../../../include -I ../../../include/CrayX1 -D Add_
           -D StringSunStyle -D F77_INTEGER=int -O 2 -h list=m
           -D LONG_IS_64BITS -h restrict=a
           -W l,--rpath=/opt/cray/cce/8.4.5/craylibs/x86-64
           -ibase-compiler /opt/cray/cce/8.4.5/CC/x86-64/compiler_include_base
           -isystem /opt/cray/cce/8.4.5/craylibs/x86-64/include
           -I /opt/gcc/4.8.1/snos/lib/gcc/x86_64-suse-linux/4.8.1/include
           -I /opt/gcc/4.8.1/snos/lib/gcc/x86_64-suse-linux/4.8.1/include-fixed
           -isystem /usr/include
           -I /opt/cray/mpt/7.3.2/gni/mpich-cray/8.3/include
           -I /opt/cray/libsci/16.03.1/CRAY/8.3/x86_64/include
           -I /opt/cray/rca/1.0.0-2.0502.60530.1.62.ari/include
           -I /opt/cray/pmi/5.0.10-1.0000.11050.0.0.ari/include
           -I /opt/cray/xpmem/0.1-2.0502.64982.5.3.ari/include
           -I /opt/cray/dmapp/7.0.1-1.0502.11080.8.76.ari/include
           -I /opt/cray/gni-headers/4.0-1.0502.10859.7.8.ari/include
           -I /opt/cray/ugni/6.0-1.0502.10863.8.29.ari/include
           -I /opt/cray/udreg/2.3.2-1.0502.10518.2.17.ari/include
           -I /opt/cray/cce/8.4.5/craylibs/x86-64/pkgconfig/../include
           -I /opt/cray/cce/8.4.5/craylibs/x86-64/pkgconfig/..//include
           -I /opt/cray/alps/5.2.4-2.0502.9774.31.11.ari/include
           -I /opt/cray/wlm_detect/1.0-1.0502.64649.2.1.ari/include
           -I /opt/cray/alps/5.2.4-2.0502.9774.31.11.ari/include
           -I /opt/cray/krca/1.0.0-2.0502.63139.4.31.ari/include
           -I /opt/cray-hss-devel/7.2.0/include

clx report
------------
Source   : /lustre/tetyda/home/lgorski/okeanos_scripts/randomaccess/hpcc-1.4.3_src_mod_ra/hpl/lib/arch/build/../../../../RandomAccess/MPIRandomAccess.c
Date     : 03/19/2016  13:20:08


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


     %%%    L o o p m a r k   L e g e n d    %%%

     Primary Loop Type        Modifiers
     ------- ---- ----        ---------
     A - Pattern matched      a - atomic memory operation
                              b - blocked
     C - Collapsed            c - conditional and/or computed
     D - Deleted               
     E - Cloned                
     F - Flat - No calls      f - fused
     G - Accelerated          g - partitioned
     I - Inlined              i - interchanged
     M - Multithreaded        m - partitioned
                              n - non-blocking remote transfer
                              p - partial
                              r - unrolled
                              s - shortloop
     V - Vectorized           w - unwound

     + - More messages listed at end of listing
     ------------------------------------------


    1.              /* -*- mode: C; tab-width: 2; indent-tabs-mode: nil; -*- */
    2.              
    3.              /*
    4.               * This code has been contributed by the DARPA HPCS program.  Contact
    5.               * David Koester <dkoester@mitre.org> or Bob Lucas <rflucas@isi.edu>
    6.               * if you have questions.
    7.               *
    8.               *
    9.               * GUPS (Giga UPdates per Second) is a measurement that profiles the memory
   10.               * architecture of a system and is a measure of performance similar to MFLOPS.
   11.               * The HPCS HPCchallenge RandomAccess benchmark is intended to exercise the
   12.               * GUPS capability of a system, much like the LINPACK benchmark is intended to
   13.               * exercise the MFLOPS capability of a computer.  In each case, we would
   14.               * expect these benchmarks to achieve close to the "peak" capability of the
   15.               * memory system. The extent of the similarities between RandomAccess and
   16.               * LINPACK are limited to both benchmarks attempting to calculate a peak system
   17.               * capability.
   18.               *
   19.               * GUPS is calculated by identifying the number of memory locations that can be
   20.               * randomly updated in one second, divided by 1 billion (1e9). The term "randomly"
   21.               * means that there is little relationship between one address to be updated and
   22.               * the next, except that they occur in the space of one half the total system
   23.               * memory.  An update is a read-modify-write operation on a table of 64-bit words.
   24.               * An address is generated, the value at that address read from memory, modified
   25.               * by an integer operation (add, and, or, xor) with a literal value, and that
   26.               * new value is written back to memory.
   27.               *
   28.               * We are interested in knowing the GUPS performance of both entire systems and
   29.               * system subcomponents --- e.g., the GUPS rating of a distributed memory
   30.               * multiprocessor the GUPS rating of an SMP node, and the GUPS rating of a
   31.               * single processor.  While there is typically a scaling of FLOPS with processor
   32.               * count, a similar phenomenon may not always occur for GUPS.
   33.               *
   34.               * Select the memory size to be the power of two such that 2^n <= 1/2 of the
   35.               * total memory.  Each CPU operates on its own address stream, and the single
   36.               * table may be distributed among nodes. The distribution of memory to nodes
   37.               * is left to the implementer.  A uniform data distribution may help balance
   38.               * the workload, while non-uniform data distributions may simplify the
   39.               * calculations that identify processor location by eliminating the requirement
   40.               * for integer divides. A small (less than 1%) percentage of missed updates
   41.               * are permitted.
   42.               *
   43.               * When implementing a benchmark that measures GUPS on a distributed memory
   44.               * multiprocessor system, it may be required to define constraints as to how
   45.               * far in the random address stream each node is permitted to "look ahead".
   46.               * Likewise, it may be required to define a constraint as to the number of
   47.               * update messages that can be stored before processing to permit multi-level
   48.               * parallelism for those systems that support such a paradigm.  The limits on
   49.               * "look ahead" and "stored updates" are being implemented to assure that the
   50.               * benchmark meets the intent to profile memory architecture and not induce
   51.               * significant artificial data locality. For the purpose of measuring GUPS,
   52.               * we will stipulate that each process is permitted to look ahead no more than
   53.               * 1024 random address stream samples with the same number of update messages
   54.               * stored before processing.
   55.               *
   56.               * The supplied MPI-1 code generates the input stream {A} on all processors
   57.               * and the global table has been distributed as uniformly as possible to
   58.               * balance the workload and minimize any Amdahl fraction.  This code does not
   59.               * exploit "look-ahead".  Addresses are sent to the appropriate processor
   60.               * where the table entry resides as soon as each address is calculated.
   61.               * Updates are performed as addresses are received.  Each message is limited
   62.               * to a single 64 bit long integer containing element ai from {A}.
   63.               * Local offsets for T[ ] are extracted by the destination processor.
   64.               *
   65.               * If the number of processors is equal to a power of two, then the global
   66.               * table can be distributed equally over the processors.  In addition, the
   67.               * processor number can be determined from that portion of the input stream
   68.               * that identifies the address into the global table by masking off log2(p)
   69.               * bits in the address.
   70.               *
   71.               * If the number of processors is not equal to a power of two, then the global
   72.               * table cannot be equally distributed between processors.  In the MPI-1
   73.               * implementation provided, there has been an attempt to minimize the differences
   74.               * in workloads and the largest difference in elements of T[ ] is one.  The
   75.               * number of values in the input stream generated by each processor will be
   76.               * related to the number of global table entries on each processor.
   77.               *
   78.               * The MPI-1 version of RandomAccess treats the potential instance where the
   79.               * number of processors is a power of two as a special case, because of the
   80.               * significant simplifications possible because processor location and local
   81.               * offset can be determined by applying masks to the input stream values.
   82.               * The non power of two case uses an integer division to determine the processor
   83.               * location.  The integer division will be more costly in terms of machine
   84.               * cycles to perform than the bit masking operations
   85.               *
   86.               * For additional information on the GUPS metric, the HPCchallenge RandomAccess
   87.               * Benchmark,and the rules to run RandomAccess or modify it to optimize
   88.               * performance -- see http://icl.cs.utk.edu/hpcc/
   89.               *
   90.               */
   91.              
   92.              /* Jan 2005
   93.               *
   94.               * This code has been modified to allow local bucket sorting of updates.
   95.               * The total maximum number of updates in the local buckets of a process
   96.               * is currently defined in "RandomAccess.h" as MAX_TOTAL_PENDING_UPDATES.
   97.               * When the total maximum number of updates is reached, the process selects
   98.               * the bucket (or destination process) with the largest number of
   99.               * updates and sends out all the updates in that bucket. See buckets.c
  100.               * for details about the buckets' implementation.
  101.               *
  102.               * This code also supports posting multiple MPI receive descriptors (based
  103.               * on a contribution by David Addison).
  104.               *
  105.               * In addition, this implementation provides an option for limiting
  106.               * the execution time of the benchmark to a specified time bound
  107.               * (see time_bound.c). The time bound is currently defined in
  108.               * time_bound.h, but it should be a benchmark parameter. By default
  109.               * the benchmark will execute the recommended number of updates,
  110.               * that is, four times the global table size.
  111.               */
  112.              
  113.              
  114.              #include <hpcc.h>
  115.              
  116.              #include "RandomAccess.h"
  117.              #include "buckets.h"
  118.              #include "time_bound.h"
  119.              
  120.              /* Allocate main table (in global memory) */
  121.              u64Int *HPCC_Table;
  122.              
  123.              u64Int LocalSendBuffer[LOCAL_BUFFER_SIZE];
  124.              u64Int LocalRecvBuffer[MAX_RECV*LOCAL_BUFFER_SIZE];
  125.              
  126.              #ifndef LONG_IS_64BITS
  127.              static void
  128.              Sum64(void *invec, void *inoutvec, int *len, MPI_Datatype *datatype) {
  129.                int i, n = *len; s64Int *invec64 = (s64Int *)invec, *inoutvec64 = (s64Int *)inoutvec;
  130.                for (i = n; i; i--, invec64++, inoutvec64++) *inoutvec64 += *invec64;
  131.              }
  132.              
  133.              static void
  134.              MinInt64(void *invec, void *inoutvec, int *len, MPI_Datatype *datatype) {
  135.                int i, n = *len; s64Int *invec64 = (s64Int *)invec, *inoutvec64 = (s64Int *)inoutvec, min_val;
  136.                if (datatype)
  137.                  for (i = n; i; i--, invec64++, inoutvec64++) {
  138.                    min_val = inoutvec64[0];
  139.                    *inoutvec64 = *inoutvec64 > *invec64 ? *invec64 : *inoutvec64;
  140.                  }
  141.              }
  142.              #endif
  143.              
  144.              #ifdef HPCC_RA_STDALG
  145.              void
  146.              AnyNodesMPIRandomAccessUpdate(HPCC_RandomAccess_tabparams_t tparams) {
  147.                s64Int i, j;
  148.                int proc_count;
  149.              
  150.                s64Int SendCnt;
  151.                u64Int Ran;
  152.                s64Int WhichPe;
  153.                u64Int GlobalOffset, LocalOffset;
  154.                int NumberReceiving = tparams.NumProcs - 1;
  155.              #ifdef USE_MULTIPLE_RECV
  156.                int index, NumRecvs;
  157.                MPI_Request inreq[MAX_RECV]  = { MPI_REQUEST_NULL };
  158.                MPI_Request outreq = MPI_REQUEST_NULL;
  159.              #else
  160.                MPI_Request inreq, outreq = MPI_REQUEST_NULL;
  161.              #endif
  162.                u64Int inmsg;
  163.                int bufferBase;
  164.              
  165.                MPI_Status status;
  166.                int have_done;
  167.              
  168.                int pe;
  169.                int pendingUpdates;
  170.                int maxPendingUpdates;
  171.                int localBufferSize;
  172.                int peUpdates;
  173.                int recvUpdates;
  174.                Bucket_Ptr Buckets;
  175.              
  176.                pendingUpdates = 0;
  177.                maxPendingUpdates = MAX_TOTAL_PENDING_UPDATES;
  178.                localBufferSize = LOCAL_BUFFER_SIZE;
  179.  +             Buckets = HPCC_InitBuckets(tparams.NumProcs, maxPendingUpdates);
  180.              
  181.                /* Perform updates to main table.  The scalar equivalent is:
  182.                 *
  183.                 *     u64Int Ran;
  184.                 *     Ran = 1;
  185.                 *     for (i=0; i<NUPDATE; i++) {
  186.                 *       Ran = (Ran << 1) ^ (((s64Int) Ran < 0) ? POLY : 0);
  187.                 *       Table[Ran & (TABSIZE-1)] ^= Ran;
  188.                 *     }
  189.                 */
  190.              
  191.                SendCnt = tparams.ProcNumUpdates; /* SendCnt = (4 * LocalTableSize); */
  192.  +             Ran = HPCC_starts (4 * tparams.GlobalStartMyProc);
  193.              
  194.                i = 0;
  195.              
  196.              #ifdef USE_MULTIPLE_RECV
  197.                NumRecvs = (tparams.NumProcs > 4) ? (Mmin(4,MAX_RECV)) : 1;
  198.  + 1-------<   for (j = 0; j < NumRecvs; j++)
  199.    1             MPI_Irecv(&LocalRecvBuffer[j*LOCAL_BUFFER_SIZE], localBufferSize,
  200.    1                       tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  201.    1------->               &inreq[j]);
  202.              #else
  203.                MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  204.                          MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  205.              #endif
  206.              
  207.  + 1-------<   while (i < SendCnt) {
  208.    1         
  209.    1             /* receive messages */
  210.  + 1 2-----<     do {
  211.    1 2       #ifdef USE_MULTIPLE_RECV
  212.  + 1 2             MPI_Testany(NumRecvs, inreq, &index, &have_done, &status);
  213.    1 2       #else
  214.    1 2             MPI_Test(&inreq, &have_done, &status);
  215.    1 2       #endif
  216.    1 2             if (have_done) {
  217.    1 2               if (status.MPI_TAG == UPDATE_TAG) {
  218.  + 1 2                 MPI_Get_count(&status, tparams.dtype64, &recvUpdates);
  219.    1 2       #ifdef USE_MULTIPLE_RECV
  220.    1 2                 bufferBase = index*LOCAL_BUFFER_SIZE;
  221.    1 2       #else
  222.    1 2                 bufferBase = 0;
  223.    1 2       #endif
  224.  + 1 2 r4--<           for (j=0; j < recvUpdates; j ++) {
  225.    1 2 r4                inmsg = LocalRecvBuffer[bufferBase+j];
  226.    1 2 r4                LocalOffset = (inmsg & (tparams.TableSize - 1)) - tparams.GlobalStartMyProc;
  227.    1 2 r4                HPCC_Table[LocalOffset] ^= inmsg;
  228.    1 2 r4-->           }
  229.    1 2       
  230.    1 2               } else if (status.MPI_TAG == FINISHED_TAG) {
  231.    1 2                 /* we got a done message.  Thanks for playing... */
  232.    1 2                 NumberReceiving--;
  233.    1 2               } else {
  234.  + 1 2                 MPI_Abort( MPI_COMM_WORLD, -1 );
  235.    1 2               }
  236.    1 2       #ifdef USE_MULTIPLE_RECV
  237.    1 2               MPI_Irecv(&LocalRecvBuffer[index*LOCAL_BUFFER_SIZE], localBufferSize,
  238.    1 2                         tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  239.    1 2                         &inreq[index]);
  240.    1 2       #else
  241.    1 2               MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  242.    1 2                         MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  243.    1 2       #endif
  244.    1 2             }
  245.    1 2----->     } while (have_done && NumberReceiving > 0);
  246.    1         
  247.    1         
  248.    1             if (pendingUpdates < maxPendingUpdates) {
  249.    1               Ran = (Ran << 1) ^ ((s64Int) Ran < ZERO64B ? POLY : ZERO64B);
  250.    1               GlobalOffset = Ran & (tparams.TableSize-1);
  251.    1               if ( GlobalOffset < tparams.Top)
  252.    1                 WhichPe = ( GlobalOffset / (tparams.MinLocalTableSize + 1) );
  253.    1               else
  254.    1                 WhichPe = ( (GlobalOffset - tparams.Remainder) / tparams.MinLocalTableSize );
  255.    1         
  256.    1               if (WhichPe == tparams.MyProc) {
  257.    1                 LocalOffset = (Ran & (tparams.TableSize - 1)) - tparams.GlobalStartMyProc;
  258.    1                 HPCC_Table[LocalOffset] ^= Ran;
  259.    1               }
  260.    1               else {
  261.  + 1                 HPCC_InsertUpdate(Ran, WhichPe, Buckets);
  262.    1                 pendingUpdates++;
  263.    1               }
  264.    1               i++;
  265.    1             }
  266.    1         
  267.    1             else {
  268.  + 1               MPI_Test(&outreq, &have_done, MPI_STATUS_IGNORE);
  269.    1               if (have_done) {
  270.    1                 outreq = MPI_REQUEST_NULL;
  271.  + 1                 pe = HPCC_GetUpdates(Buckets, LocalSendBuffer, localBufferSize, &peUpdates);
  272.    1                 MPI_Isend(&LocalSendBuffer, peUpdates, tparams.dtype64, (int)pe, UPDATE_TAG,
  273.    1                           MPI_COMM_WORLD, &outreq);
  274.    1                 pendingUpdates -= peUpdates;
  275.    1               }
  276.    1             }
  277.    1         
  278.    1------->   }
  279.              
  280.                /* send remaining updates in buckets */
  281.  + 1-------<   while (pendingUpdates > 0) {
  282.    1         
  283.    1             /* receive messages */
  284.  + 1 2-----<     do {
  285.    1 2       #ifdef USE_MULTIPLE_RECV
  286.  + 1 2             MPI_Testany(NumRecvs, inreq, &index, &have_done, &status);
  287.    1 2       #else
  288.    1 2             MPI_Test(&inreq, &have_done, &status);
  289.    1 2       #endif
  290.    1 2             if (have_done) {
  291.    1 2               if (status.MPI_TAG == UPDATE_TAG) {
  292.  + 1 2                 MPI_Get_count(&status, tparams.dtype64, &recvUpdates);
  293.    1 2       #ifdef USE_MULTIPLE_RECV
  294.    1 2                 bufferBase = index*LOCAL_BUFFER_SIZE;
  295.    1 2       #else
  296.    1 2                 bufferBase = 0;
  297.    1 2       #endif
  298.  + 1 2 r4--<           for (j=0; j < recvUpdates; j ++) {
  299.    1 2 r4                inmsg = LocalRecvBuffer[bufferBase+j];
  300.    1 2 r4                LocalOffset = (inmsg & (tparams.TableSize - 1)) - tparams.GlobalStartMyProc;
  301.    1 2 r4                HPCC_Table[LocalOffset] ^= inmsg;
  302.    1 2 r4-->           }
  303.    1 2       
  304.    1 2               } else if (status.MPI_TAG == FINISHED_TAG) {
  305.    1 2                 /* we got a done message.  Thanks for playing... */
  306.    1 2                 NumberReceiving--;
  307.    1 2               } else {
  308.  + 1 2                 MPI_Abort( MPI_COMM_WORLD, -1 );
  309.    1 2               }
  310.    1 2       #ifdef USE_MULTIPLE_RECV
  311.    1 2               MPI_Irecv(&LocalRecvBuffer[index*LOCAL_BUFFER_SIZE], localBufferSize,
  312.    1 2                         tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  313.    1 2                         &inreq[index]);
  314.    1 2       #else
  315.    1 2               MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  316.    1 2                         MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  317.    1 2       #endif
  318.    1 2             }
  319.    1 2----->     } while (have_done && NumberReceiving > 0);
  320.    1         
  321.    1         
  322.  + 1             MPI_Test(&outreq, &have_done, MPI_STATUS_IGNORE);
  323.    1             if (have_done) {
  324.    1               outreq = MPI_REQUEST_NULL;
  325.  + 1               pe = HPCC_GetUpdates(Buckets, LocalSendBuffer, localBufferSize, &peUpdates);
  326.    1               MPI_Isend(&LocalSendBuffer, peUpdates, tparams.dtype64, (int)pe, UPDATE_TAG,
  327.    1                         MPI_COMM_WORLD, &outreq);
  328.    1               pendingUpdates -= peUpdates;
  329.    1             }
  330.    1         
  331.    1------->   }
  332.              
  333.                /* send our done messages */
  334.  + 1-------<   for (proc_count = 0 ; proc_count < tparams.NumProcs ; ++proc_count) {
  335.    1             if (proc_count == tparams.MyProc) { tparams.finish_req[tparams.MyProc] = MPI_REQUEST_NULL; continue; }
  336.    1             /* send garbage - who cares, no one will look at it */
  337.    1             MPI_Isend(&Ran, 0, tparams.dtype64, proc_count, FINISHED_TAG,
  338.    1                       MPI_COMM_WORLD, tparams.finish_req + proc_count);
  339.    1------->   }
  340.              
  341.                /* Finish everyone else up... */
  342.  + 1-------<   while (NumberReceiving > 0) {
  343.    1         #ifdef USE_MULTIPLE_RECV
  344.  + 1             MPI_Waitany(NumRecvs, inreq, &index, &status);
  345.    1         #else
  346.    1             MPI_Wait(&inreq, &status);
  347.    1         #endif
  348.    1             if (status.MPI_TAG == UPDATE_TAG) {
  349.  + 1               MPI_Get_count(&status, tparams.dtype64, &recvUpdates);
  350.    1         #ifdef USE_MULTIPLE_RECV
  351.    1               bufferBase = index * LOCAL_BUFFER_SIZE;
  352.    1         #else
  353.    1               bufferBase = 0;
  354.    1         #endif
  355.  + 1 r4----<       for (j=0; j < recvUpdates; j ++) {
  356.    1 r4              inmsg = LocalRecvBuffer[bufferBase+j];
  357.    1 r4              LocalOffset = (inmsg & (tparams.TableSize - 1)) - tparams.GlobalStartMyProc;
  358.    1 r4              HPCC_Table[LocalOffset] ^= inmsg;
  359.    1 r4---->       }
  360.    1         
  361.    1             } else if (status.MPI_TAG == FINISHED_TAG) {
  362.    1               /* we got a done message.  Thanks for playing... */
  363.    1               NumberReceiving--;
  364.    1             } else {
  365.  + 1               MPI_Abort( MPI_COMM_WORLD, -1 );
  366.    1             }
  367.    1         #ifdef USE_MULTIPLE_RECV
  368.    1             MPI_Irecv(&LocalRecvBuffer[index*LOCAL_BUFFER_SIZE], localBufferSize,
  369.    1                       tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  370.    1                       &inreq[index]);
  371.    1         #else
  372.    1             MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  373.    1                       MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  374.    1         #endif
  375.    1------->   }
  376.              
  377.                MPI_Waitall( tparams.NumProcs, tparams.finish_req, tparams.finish_statuses);
  378.              
  379.                /* Be nice and clean up after ourselves */
  380.  +             HPCC_FreeBuckets(Buckets, tparams.NumProcs);
  381.              #ifdef USE_MULTIPLE_RECV
  382.  + 1-------<   for (j = 0; j < NumRecvs; j++) {
  383.  + 1             MPI_Cancel(&inreq[j]);
  384.    1             MPI_Wait(&inreq[j], MPI_STATUS_IGNORE);
  385.    1------->   }
  386.              #else
  387.                MPI_Cancel(&inreq);
  388.                MPI_Wait(&inreq, MPI_STATUS_IGNORE);
  389.              #endif
  390.                MPI_Wait(&outreq, MPI_STATUS_IGNORE);
  391.              
  392.                /* end multiprocessor code */
  393.              }
  394.              
  395.              void
  396.              Power2NodesMPIRandomAccessUpdate(HPCC_RandomAccess_tabparams_t tparams) {
  397.                s64Int i, j;
  398.                int proc_count;
  399.              
  400.                s64Int SendCnt;
  401.                u64Int Ran;
  402.                s64Int WhichPe;
  403.                u64Int LocalOffset;
  404.                int logLocalTableSize = tparams.logTableSize - tparams.logNumProcs;
  405.                int NumberReceiving = tparams.NumProcs - 1;
  406.              #ifdef USE_MULTIPLE_RECV
  407.                int index, NumRecvs;
  408.                MPI_Request inreq[MAX_RECV]  = { MPI_REQUEST_NULL };
  409.                MPI_Request outreq = MPI_REQUEST_NULL;
  410.              #else
  411.                MPI_Request inreq, outreq = MPI_REQUEST_NULL;
  412.              #endif
  413.                u64Int inmsg;
  414.                int bufferBase;
  415.              
  416.                MPI_Status status;
  417.                int have_done;
  418.              
  419.                int pe;
  420.                int pendingUpdates;
  421.                int maxPendingUpdates;
  422.                int localBufferSize;
  423.                int peUpdates;
  424.                int recvUpdates;
  425.                Bucket_Ptr Buckets;
  426.              
  427.                pendingUpdates = 0;
  428.                maxPendingUpdates = MAX_TOTAL_PENDING_UPDATES;
  429.                localBufferSize = LOCAL_BUFFER_SIZE;
  430.  +             Buckets = HPCC_InitBuckets(tparams.NumProcs, maxPendingUpdates);
  431.              
  432.                /* Perform updates to main table.  The scalar equivalent is:
  433.                 *
  434.                 *     u64Int Ran;
  435.                 *     Ran = 1;
  436.                 *     for (i=0; i<NUPDATE; i++) {
  437.                 *       Ran = (Ran << 1) ^ (((s64Int) Ran < 0) ? POLY : 0);
  438.                 *       Table[Ran & (TABSIZE-1)] ^= Ran;
  439.                 *     }
  440.                 */
  441.              
  442.                SendCnt = tparams.ProcNumUpdates; /*  SendCnt = (4 * LocalTableSize); */
  443.  +             Ran = HPCC_starts (4 * tparams.GlobalStartMyProc);
  444.              
  445.                i = 0;
  446.              
  447.              #ifdef USE_MULTIPLE_RECV
  448.                NumRecvs = (tparams.NumProcs > 4) ? (Mmin(4,MAX_RECV)) : 1;
  449.  + 1-------<   for (j = 0; j < NumRecvs; j++)
  450.    1             MPI_Irecv(&LocalRecvBuffer[j*LOCAL_BUFFER_SIZE], localBufferSize,
  451.    1                       tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  452.    1------->               &inreq[j]);
  453.              #else
  454.                MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  455.                          MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  456.              #endif
  457.              
  458.  + 1-------<   while (i < SendCnt) {
  459.    1         
  460.    1             /* receive messages */
  461.  + 1 2-----<     do {
  462.    1 2       #ifdef USE_MULTIPLE_RECV
  463.  + 1 2             MPI_Testany(NumRecvs, inreq, &index, &have_done, &status);
  464.    1 2       #else
  465.    1 2             MPI_Test(&inreq, &have_done, &status);
  466.    1 2       #endif
  467.    1 2             if (have_done) {
  468.    1 2               if (status.MPI_TAG == UPDATE_TAG) {
  469.  + 1 2                 MPI_Get_count(&status, tparams.dtype64, &recvUpdates);
  470.    1 2       #ifdef USE_MULTIPLE_RECV
  471.    1 2                 bufferBase = index * LOCAL_BUFFER_SIZE;
  472.    1 2       #else
  473.    1 2                 bufferBase = 0;
  474.    1 2       #endif
  475.  + 1 2 r4--<           for (j=0; j < recvUpdates; j ++) {
  476.    1 2 r4                inmsg = LocalRecvBuffer[bufferBase+j];
  477.    1 2 r4                HPCC_Table[inmsg & (tparams.LocalTableSize-1)] ^= inmsg;
  478.    1 2 r4-->           }
  479.    1 2       
  480.    1 2               } else if (status.MPI_TAG == FINISHED_TAG) {
  481.    1 2                 /* we got a done message.  Thanks for playing... */
  482.    1 2                 NumberReceiving--;
  483.    1 2               } else {
  484.  + 1 2                 MPI_Abort( MPI_COMM_WORLD, -1 );
  485.    1 2               }
  486.    1 2       #ifdef USE_MULTIPLE_RECV
  487.    1 2               MPI_Irecv(&LocalRecvBuffer[index*LOCAL_BUFFER_SIZE], localBufferSize,
  488.    1 2                         tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  489.    1 2                         &inreq[index]);
  490.    1 2       #else
  491.    1 2               MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  492.    1 2                         MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  493.    1 2       #endif
  494.    1 2             }
  495.    1 2----->     } while (have_done && NumberReceiving > 0);
  496.    1         
  497.    1         
  498.    1             if (pendingUpdates < maxPendingUpdates) {
  499.    1               Ran = (Ran << 1) ^ ((s64Int) Ran < ZERO64B ? POLY : ZERO64B);
  500.    1               WhichPe = (Ran >> logLocalTableSize) & (tparams.NumProcs - 1);
  501.    1               if (WhichPe == tparams.MyProc) {
  502.    1                 LocalOffset = (Ran & (tparams.TableSize - 1)) - tparams.GlobalStartMyProc;
  503.    1                 HPCC_Table[LocalOffset] ^= Ran;
  504.    1               }
  505.    1               else {
  506.  + 1                 HPCC_InsertUpdate(Ran, WhichPe, Buckets);
  507.    1                 pendingUpdates++;
  508.    1               }
  509.    1               i++;
  510.    1             }
  511.    1         
  512.    1             else {
  513.  + 1               MPI_Test(&outreq, &have_done, MPI_STATUS_IGNORE);
  514.    1               if (have_done) {
  515.    1                 outreq = MPI_REQUEST_NULL;
  516.  + 1                 pe = HPCC_GetUpdates(Buckets, LocalSendBuffer, localBufferSize, &peUpdates);
  517.    1                 MPI_Isend(&LocalSendBuffer, peUpdates, tparams.dtype64, (int)pe, UPDATE_TAG,
  518.    1                           MPI_COMM_WORLD, &outreq);
  519.    1                 pendingUpdates -= peUpdates;
  520.    1               }
  521.    1             }
  522.    1         
  523.    1------->   }
  524.              
  525.              
  526.                /* send remaining updates in buckets */
  527.  + 1-------<   while (pendingUpdates > 0) {
  528.    1         
  529.    1             /* receive messages */
  530.  + 1 2-----<     do {
  531.    1 2       #ifdef USE_MULTIPLE_RECV
  532.  + 1 2             MPI_Testany(NumRecvs, inreq, &index, &have_done, &status);
  533.    1 2       #else
  534.    1 2             MPI_Test(&inreq, &have_done, &status);
  535.    1 2       #endif
  536.    1 2             if (have_done) {
  537.    1 2               if (status.MPI_TAG == UPDATE_TAG) {
  538.  + 1 2                 MPI_Get_count(&status, tparams.dtype64, &recvUpdates);
  539.    1 2       #ifdef USE_MULTIPLE_RECV
  540.    1 2                 bufferBase = index * LOCAL_BUFFER_SIZE;
  541.    1 2       #else
  542.    1 2                 bufferBase = 0;
  543.    1 2       #endif
  544.  + 1 2 r4--<           for (j=0; j < recvUpdates; j ++) {
  545.    1 2 r4                inmsg = LocalRecvBuffer[bufferBase+j];
  546.    1 2 r4                HPCC_Table[inmsg & (tparams.LocalTableSize-1)] ^= inmsg;
  547.    1 2 r4-->           }
  548.    1 2               } else if (status.MPI_TAG == FINISHED_TAG) {
  549.    1 2                 /* we got a done message.  Thanks for playing... */
  550.    1 2                 NumberReceiving--;
  551.    1 2               } else {
  552.  + 1 2                 MPI_Abort( MPI_COMM_WORLD, -1 );
  553.    1 2               }
  554.    1 2       #ifdef USE_MULTIPLE_RECV
  555.    1 2               MPI_Irecv(&LocalRecvBuffer[index*LOCAL_BUFFER_SIZE], localBufferSize,
  556.    1 2                         tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  557.    1 2                         &inreq[index]);
  558.    1 2       #else
  559.    1 2               MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  560.    1 2                         MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  561.    1 2       #endif
  562.    1 2             }
  563.    1 2----->     } while (have_done && NumberReceiving > 0);
  564.    1         
  565.    1         
  566.  + 1             MPI_Test(&outreq, &have_done, MPI_STATUS_IGNORE);
  567.    1             if (have_done) {
  568.    1               outreq = MPI_REQUEST_NULL;
  569.  + 1               pe = HPCC_GetUpdates(Buckets, LocalSendBuffer, localBufferSize, &peUpdates);
  570.    1               MPI_Isend(&LocalSendBuffer, peUpdates, tparams.dtype64, (int)pe, UPDATE_TAG,
  571.    1                         MPI_COMM_WORLD, &outreq);
  572.    1               pendingUpdates -= peUpdates;
  573.    1             }
  574.    1         
  575.    1------->   }
  576.              
  577.                /* send our done messages */
  578.  + 1-------<   for (proc_count = 0 ; proc_count < tparams.NumProcs ; ++proc_count) {
  579.    1             if (proc_count == tparams.MyProc) { tparams.finish_req[tparams.MyProc] = MPI_REQUEST_NULL; continue; }
  580.    1             /* send garbage - who cares, no one will look at it */
  581.    1             MPI_Isend(&Ran, 0, tparams.dtype64, proc_count, FINISHED_TAG,
  582.    1                       MPI_COMM_WORLD, tparams.finish_req + proc_count);
  583.    1------->   }
  584.              
  585.                /* Finish everyone else up... */
  586.  + 1-------<   while (NumberReceiving > 0) {
  587.    1         #ifdef USE_MULTIPLE_RECV
  588.  + 1             MPI_Waitany(NumRecvs, inreq, &index, &status);
  589.    1         #else
  590.    1             MPI_Wait(&inreq, &status);
  591.    1         #endif
  592.    1             if (status.MPI_TAG == UPDATE_TAG) {
  593.  + 1               MPI_Get_count(&status, tparams.dtype64, &recvUpdates);
  594.    1         #ifdef USE_MULTIPLE_RECV
  595.    1               bufferBase = index * LOCAL_BUFFER_SIZE;
  596.    1         #else
  597.    1               bufferBase = 0;
  598.    1         #endif
  599.  + 1 r4----<       for (j=0; j < recvUpdates; j ++) {
  600.    1 r4              inmsg = LocalRecvBuffer[bufferBase+j];
  601.    1 r4              HPCC_Table[inmsg & (tparams.LocalTableSize-1)] ^= inmsg;
  602.    1 r4---->       }
  603.    1         
  604.    1             } else if (status.MPI_TAG == FINISHED_TAG) {
  605.    1               /* we got a done message.  Thanks for playing... */
  606.    1               NumberReceiving--;
  607.    1             } else {
  608.  + 1               MPI_Abort( MPI_COMM_WORLD, -1 );
  609.    1             }
  610.    1         #ifdef USE_MULTIPLE_RECV
  611.    1             MPI_Irecv(&LocalRecvBuffer[index*LOCAL_BUFFER_SIZE], localBufferSize,
  612.    1                       tparams.dtype64, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD,
  613.    1                       &inreq[index]);
  614.    1         #else
  615.    1             MPI_Irecv(&LocalRecvBuffer, localBufferSize, tparams.dtype64,
  616.    1                       MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &inreq);
  617.    1         #endif
  618.    1------->   }
  619.              
  620.                MPI_Waitall( tparams.NumProcs, tparams.finish_req, tparams.finish_statuses);
  621.              
  622.                /* Be nice and clean up after ourselves */
  623.  +             HPCC_FreeBuckets(Buckets, tparams.NumProcs);
  624.              #ifdef USE_MULTIPLE_RECV
  625.  + 1-------<   for (j = 0; j < NumRecvs; j++) {
  626.  + 1             MPI_Cancel(&inreq[j]);
  627.    1             MPI_Wait(&inreq[j], MPI_STATUS_IGNORE);
  628.    1------->   }
  629.              #else
  630.                MPI_Cancel(&inreq);
  631.                MPI_Wait(&inreq, MPI_STATUS_IGNORE);
  632.              #endif
  633.                MPI_Wait(&outreq, MPI_STATUS_IGNORE);
  634.              
  635.                /* end multiprocessor code */
  636.              }
  637.              #endif
  638.              
  639.              int
  640.              HPCC_MPIRandomAccess(HPCC_Params *params) {
  641.                s64Int i, NumErrors, GlbNumErrors;
  642.              
  643.                double CPUTime;  /* CPU  time to update table */
  644.                double RealTime; /* Real time to update table */
  645.              
  646.                double TotalMem;
  647.                int sAbort, rAbort;
  648.                int PowerofTwo;
  649.              
  650.                double timeBound = -1;  /* OPTIONAL time bound for execution time */
  651.                u64Int NumUpdates_Default; /* Number of updates to table (suggested: 4x number of table entries) */
  652.                u64Int NumUpdates;  /* actual number of updates to table - may be smaller than
  653.                                     * NumUpdates_Default due to execution time bounds */
  654.              
  655.              #ifdef RA_TIME_BOUND
  656.                s64Int localProcNumUpdates, GlbNumUpdates;  /* for reduction */
  657.              #ifndef LONG_IS_64BITS
  658.                MPI_Op min_int64;
  659.              #endif
  660.              #endif
  661.              
  662.                FILE *outFile = NULL;
  663.                double *GUPs;
  664.              
  665.                HPCC_RandomAccess_tabparams_t tparams;
  666.              
  667.              #ifdef LONG_IS_64BITS
  668.                tparams.dtype64 = MPI_LONG;
  669.              #else
  670.                MPI_Op sum64;
  671.                tparams.dtype64 = MPI_LONG_LONG_INT;
  672.              #endif
  673.              
  674.                GUPs = &params->MPIRandomAccess_GUPs;
  675.              
  676.  +             MPI_Comm_size( MPI_COMM_WORLD, &tparams.NumProcs );
  677.  +             MPI_Comm_rank( MPI_COMM_WORLD, &tparams.MyProc );
  678.              
  679.                if (0 == tparams.MyProc) {
  680.  +               outFile = fopen( params->outFname, "a" );
  681.                  if (! outFile) outFile = stderr;
  682.                }
  683.              
  684.                TotalMem = params->HPLMaxProcMem; /* max single node memory */
  685.                TotalMem *= tparams.NumProcs;     /* max memory in tparams.NumProcs nodes */
  686.                TotalMem /= sizeof(u64Int);
  687.              
  688.                /* calculate TableSize --- the size of update array (must be a power of 2) */
  689.  + 1-------<   for (TotalMem *= 0.5, tparams.logTableSize = 0, tparams.TableSize = 1;
  690.    1                TotalMem >= 1.0;
  691.    1                TotalMem *= 0.5, tparams.logTableSize++, tparams.TableSize <<= 1)
  692.    1------->     ; /* EMPTY */
  693.              
  694.              
  695.                /* determine whether the number of processors is a power of 2 */
  696.  + 1-------<   for (i = 1, tparams.logNumProcs = 0; ; tparams.logNumProcs++, i <<= 1) {
  697.    1             if (i == tparams.NumProcs) {
  698.    1               PowerofTwo = HPCC_TRUE;
  699.    1               tparams.Remainder = 0;
  700.    1               tparams.Top = 0;
  701.    1               tparams.MinLocalTableSize = (tparams.TableSize / tparams.NumProcs);
  702.    1               tparams.LocalTableSize = tparams.MinLocalTableSize;
  703.    1               tparams.GlobalStartMyProc = (tparams.MinLocalTableSize * tparams.MyProc);
  704.    1               break;
  705.    1         
  706.    1             /* number of processes is not a power 2 (too many shifts may introduce negative values or 0) */
  707.    1         
  708.    1             }
  709.    1             else if (i > tparams.NumProcs || i <= 0) {
  710.    1               PowerofTwo = HPCC_FALSE;
  711.    1               /* Minimum local table size --- some processors have an additional entry */
  712.    1               tparams.MinLocalTableSize = (tparams.TableSize / tparams.NumProcs);
  713.    1               /* Number of processors with (tparams.LocalTableSize + 1) entries */
  714.    1               tparams.Remainder = tparams.TableSize  - (tparams.MinLocalTableSize * tparams.NumProcs);
  715.    1               /* Number of table entries in top of Table */
  716.    1               tparams.Top = (tparams.MinLocalTableSize + 1) * tparams.Remainder;
  717.    1               /* Local table size */
  718.    1               if (tparams.MyProc < tparams.Remainder) {
  719.    1                   tparams.LocalTableSize = (tparams.MinLocalTableSize + 1);
  720.    1                   tparams.GlobalStartMyProc = ( (tparams.MinLocalTableSize + 1) * tparams.MyProc);
  721.    1                 }
  722.    1                 else {
  723.    1                   tparams.LocalTableSize = tparams.MinLocalTableSize;
  724.    1                   tparams.GlobalStartMyProc = ( (tparams.MinLocalTableSize * tparams.MyProc) + tparams.Remainder );
  725.    1                 }
  726.    1               break;
  727.    1         
  728.    1             } /* end else if */
  729.    1------->   } /* end for i */
  730.              
  731.                sAbort = 0;
  732.                tparams.finish_statuses = XMALLOC( MPI_Status, tparams.NumProcs );
  733.                tparams.finish_req = XMALLOC( MPI_Request, tparams.NumProcs );
  734.                HPCC_Table = HPCC_XMALLOC( u64Int, tparams.LocalTableSize );
  735.              
  736.                if (! tparams.finish_statuses || ! tparams.finish_req || ! HPCC_Table) sAbort = 1;
  737.              
  738.  +             MPI_Allreduce( &sAbort, &rAbort, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );
  739.                if (rAbort > 0) {
  740.                  if (tparams.MyProc == 0) fprintf(outFile, "Failed to allocate memory for the main table.\n");
  741.                  /* check all allocations in case there are new added and their order changes */
  742.                  if (tparams.finish_statuses) free( tparams.finish_statuses );
  743.                  if (tparams.finish_req) free( tparams.finish_req );
  744.                  if (HPCC_Table) HPCC_free( HPCC_Table );
  745.              
  746.                  goto failed_table;
  747.                }
  748.              
  749.                params->MPIRandomAccess_N = (s64Int)tparams.TableSize;
  750.              
  751.                /* Default number of global updates to table: 4x number of table entries */
  752.                NumUpdates_Default = 4 * tparams.TableSize;
  753.                tparams.ProcNumUpdates = 4*tparams.LocalTableSize;
  754.                NumUpdates = NumUpdates_Default;
  755.              
  756.                /* The time bound is only accurate for standard RandomAccess algorithm. */
  757.              #ifdef HPCC_RA_STDALG
  758.              #ifdef RA_TIME_BOUND
  759.                /* estimate number of updates such that execution time does not exceed time bound */
  760.                /* time_bound should be a parameter */
  761.                /* max run time in seconds */
  762.  +             MPI_Allreduce( &params->HPLrdata.time, &timeBound, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD );
  763.                timeBound = Mmax( 0.25 * timeBound, (double)TIME_BOUND );
  764.                if (PowerofTwo) {
  765.  +               HPCC_Power2NodesTime( tparams, timeBound, (u64Int *)&localProcNumUpdates );
  766.                } else {
  767.  +               HPCC_AnyNodesTime( tparams, timeBound, (u64Int *)&localProcNumUpdates );
  768.                }
  769.                /* be conservative: get the smallest number of updates among all procs */
  770.              #ifdef LONG_IS_64BITS
  771.  +             MPI_Allreduce( &localProcNumUpdates, &GlbNumUpdates, 1, MPI_LONG, MPI_MIN, MPI_COMM_WORLD );
  772.              #else
  773.                MPI_Op_create( MinInt64, 1, &min_int64 );
  774.                MPI_Allreduce( &localProcNumUpdates, &GlbNumUpdates, 1, tparams.dtype64, min_int64, MPI_COMM_WORLD );
  775.                MPI_Op_free( &min_int64 );
  776.              #endif
  777.                tparams.ProcNumUpdates = Mmin(GlbNumUpdates, (4*tparams.LocalTableSize));
  778.                /* works for both PowerofTwo and AnyNodes */
  779.                NumUpdates = Mmin((tparams.ProcNumUpdates*tparams.NumProcs), (s64Int)NumUpdates_Default);
  780.              #endif
  781.              #endif
  782.              
  783.                if (tparams.MyProc == 0) {
  784.                  fprintf( outFile, "Running on %d processors%s\n", tparams.NumProcs, PowerofTwo ? " (PowerofTwo)" : "");
  785.                  fprintf( outFile, "Total Main table size = 2^" FSTR64 " = " FSTR64 " words\n",
  786.                           tparams.logTableSize, tparams.TableSize );
  787.                  if (PowerofTwo)
  788.                      fprintf( outFile, "PE Main table size = 2^" FSTR64 " = " FSTR64 " words/PE\n",
  789.                               (tparams.logTableSize - tparams.logNumProcs), tparams.TableSize/tparams.NumProcs );
  790.                    else
  791.                      fprintf( outFile, "PE Main table size = (2^" FSTR64 ")/%d  = " FSTR64 " words/PE MAX\n",
  792.                               tparams.logTableSize, tparams.NumProcs, tparams.LocalTableSize);
  793.              
  794.                  fprintf( outFile, "Default number of updates (RECOMMENDED) = " FSTR64 "\n", NumUpdates_Default);
  795.              #ifdef RA_TIME_BOUND
  796.                  fprintf( outFile, "Number of updates EXECUTED = " FSTR64 " (for a TIME BOUND of %.2f secs)\n",
  797.                           NumUpdates, timeBound);
  798.              #endif
  799.                  params->MPIRandomAccess_ExeUpdates = NumUpdates;
  800.                  params->MPIRandomAccess_TimeBound = timeBound;
  801.                }
  802.              
  803.                /* Initialize main table */
  804.  + 1-------<   for (i=0; i<tparams.LocalTableSize; i++)
  805.    1------->     HPCC_Table[i] = i + tparams.GlobalStartMyProc;
  806.              
  807.  +             MPI_Barrier( MPI_COMM_WORLD );
  808.              
  809.  +             CPUTime = -CPUSEC();
  810.  +             RealTime = -RTSEC();
  811.              
  812.                if (PowerofTwo) {
  813.  +               Power2NodesMPIRandomAccessUpdate( tparams );
  814.                } else {
  815.  +               AnyNodesMPIRandomAccessUpdate( tparams );
  816.                }
  817.              
  818.              
  819.  +             MPI_Barrier( MPI_COMM_WORLD );
  820.              
  821.                /* End timed section */
  822.  +             CPUTime += CPUSEC();
  823.  +             RealTime += RTSEC();
  824.              
  825.                /* Print timing results */
  826.                if (tparams.MyProc == 0){
  827.                  params->MPIRandomAccess_time = RealTime;
  828.                  *GUPs = 1e-9*NumUpdates / RealTime;
  829.                  fprintf( outFile, "CPU time used = %.6f seconds\n", CPUTime );
  830.                  fprintf( outFile, "Real time used = %.6f seconds\n", RealTime );
  831.                  fprintf( outFile, "%.9f Billion(10^9) Updates    per second [GUP/s]\n",
  832.                           *GUPs );
  833.                  fprintf( outFile, "%.9f Billion(10^9) Updates/PE per second [GUP/s]\n",
  834.                           *GUPs / tparams.NumProcs );
  835.                  /* No longer reporting per CPU number */
  836.                  /* *GUPs /= NumProcs; */
  837.                }
  838.                /* distribute result to all nodes */
  839.  +             MPI_Bcast( GUPs, 1, MPI_INT, 0, MPI_COMM_WORLD );
  840.              
  841.              
  842.                /* Verification phase */
  843.              
  844.                /* Begin timing here */
  845.  +             CPUTime = -CPUSEC();
  846.  +             RealTime = -RTSEC();
  847.              
  848.                if (PowerofTwo) {
  849.  +               HPCC_Power2NodesMPIRandomAccessCheck( tparams, &NumErrors );
  850.                }
  851.                else {
  852.  +               HPCC_AnyNodesMPIRandomAccessCheck( tparams, &NumErrors );
  853.                }
  854.              
  855.              
  856.              #ifdef LONG_IS_64BITS
  857.  +             MPI_Reduce( &NumErrors, &GlbNumErrors, 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD );
  858.              #else
  859.                /* MPI 1.1 standard (obsolete at this point) doesn't define MPI_SUM
  860.                  to work on `long long':
  861.                  http://www.mpi-forum.org/docs/mpi-11-html/node78.html and
  862.                  therefore LAM 6.5.6 chooses not to implement it (even though there
  863.                  is code for it in LAM and for other reductions work OK,
  864.                  e.g. MPI_MAX). MPICH 1.2.5 doesn't complain about MPI_SUM but it
  865.                  doesn't have MPI_UNSIGNED_LONG_LONG (but has MPI_LONG_LONG_INT):
  866.                  http://www.mpi-forum.org/docs/mpi-20-html/node84.htm So I need to
  867.                  create a trivial summation operation. */
  868.                MPI_Op_create( Sum64, 1, &sum64 );
  869.                MPI_Reduce( &NumErrors, &GlbNumErrors, 1, tparams.dtype64, sum64, 0, MPI_COMM_WORLD );
  870.                MPI_Op_free( &sum64 );
  871.              #endif
  872.              
  873.                /* End timed section */
  874.  +             CPUTime += CPUSEC();
  875.  +             RealTime += RTSEC();
  876.              
  877.                if(tparams.MyProc == 0){
  878.                  params->MPIRandomAccess_CheckTime = RealTime;
  879.                  fprintf( outFile, "Verification:  CPU time used = %.6f seconds\n", CPUTime);
  880.                  fprintf( outFile, "Verification:  Real time used = %.6f seconds\n", RealTime);
  881.                  fprintf( outFile, "Found " FSTR64 " errors in " FSTR64 " locations (%s).\n",
  882.                           GlbNumErrors, tparams.TableSize, (GlbNumErrors <= 0.01*tparams.TableSize) ?
  883.                           "passed" : "failed");
  884.                  if (GlbNumErrors > 0.01*tparams.TableSize) params->Failure = 1;
  885.                  params->MPIRandomAccess_Errors = (s64Int)GlbNumErrors;
  886.                  params->MPIRandomAccess_ErrorsFraction = (double)GlbNumErrors / (double)tparams.TableSize;
  887.                  params->MPIRandomAccess_Algorithm = HPCC_RA_ALGORITHM;
  888.                }
  889.                /* End verification phase */
  890.              
  891.              
  892.                /* Deallocate memory (in reverse order of allocation which should
  893.                   help fragmentation) */
  894.              
  895.                HPCC_free( HPCC_Table );
  896.                free( tparams.finish_req );
  897.                free( tparams.finish_statuses );
  898.              
  899.                failed_table:
  900.              
  901.  +             if (0 == tparams.MyProc) if (outFile != stderr) fclose( outFile );
  902.              
  903.  +             MPI_Barrier( MPI_COMM_WORLD );
  904.              
  905.                return 0;
  906.              }

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 179 
  "HPCC_InitBuckets" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 192 
  "HPCC_starts" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 198 
  A loop was not vectorized because it contains a call to function "MPI_Irecv" on line 199.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 210 
  A loop was not vectorized because it contains a call to function "MPI_Testany" on line 212.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 212 
  "MPI_Testany" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 218 
  "MPI_Get_count" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6005 CC: SCALAR File = MPIRandomAccess.c, Line = 224 
  A loop was unrolled 4 times.

CC-6291 CC: VECTOR File = MPIRandomAccess.c, Line = 224 
  A loop was not vectorized because a recurrence was found between "LocalRecvBuffer" at line 225 and "HPCC_Table" at line 227.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 234 
  "MPI_Abort" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 261 
  "HPCC_InsertUpdate" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 268 
  "MPI_Test" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 271 
  "HPCC_GetUpdates" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 284 
  A loop was not vectorized because it contains a call to function "MPI_Testany" on line 286.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 286 
  "MPI_Testany" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 292 
  "MPI_Get_count" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6005 CC: SCALAR File = MPIRandomAccess.c, Line = 298 
  A loop was unrolled 4 times.

CC-6291 CC: VECTOR File = MPIRandomAccess.c, Line = 298 
  A loop was not vectorized because a recurrence was found between "LocalRecvBuffer" at line 299 and "HPCC_Table" at line 301.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 308 
  "MPI_Abort" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 322 
  "MPI_Test" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 325 
  "HPCC_GetUpdates" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 334 
  A loop was not vectorized because it contains a call to function "MPI_Isend" on line 337.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 342 
  A loop was not vectorized because it contains a call to function "MPI_Waitany" on line 344.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 344 
  "MPI_Waitany" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 349 
  "MPI_Get_count" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6005 CC: SCALAR File = MPIRandomAccess.c, Line = 355 
  A loop was unrolled 4 times.

CC-6291 CC: VECTOR File = MPIRandomAccess.c, Line = 355 
  A loop was not vectorized because a recurrence was found between "LocalRecvBuffer" at line 356 and "HPCC_Table" at line 358.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 365 
  "MPI_Abort" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 380 
  "HPCC_FreeBuckets" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 382 
  A loop was not vectorized because it contains a call to function "MPI_Cancel" on line 383.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 383 
  "MPI_Cancel" (called from "AnyNodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 430 
  "HPCC_InitBuckets" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 443 
  "HPCC_starts" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 449 
  A loop was not vectorized because it contains a call to function "MPI_Irecv" on line 450.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 461 
  A loop was not vectorized because it contains a call to function "MPI_Testany" on line 463.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 463 
  "MPI_Testany" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 469 
  "MPI_Get_count" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6005 CC: SCALAR File = MPIRandomAccess.c, Line = 475 
  A loop was unrolled 4 times.

CC-6291 CC: VECTOR File = MPIRandomAccess.c, Line = 475 
  A loop was not vectorized because a recurrence was found between "LocalRecvBuffer" at line 476 and "HPCC_Table" at line 477.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 484 
  "MPI_Abort" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 506 
  "HPCC_InsertUpdate" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate
  the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 513 
  "MPI_Test" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 516 
  "HPCC_GetUpdates" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 530 
  A loop was not vectorized because it contains a call to function "MPI_Testany" on line 532.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 532 
  "MPI_Testany" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 538 
  "MPI_Get_count" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6005 CC: SCALAR File = MPIRandomAccess.c, Line = 544 
  A loop was unrolled 4 times.

CC-6291 CC: VECTOR File = MPIRandomAccess.c, Line = 544 
  A loop was not vectorized because a recurrence was found between "LocalRecvBuffer" at line 545 and "HPCC_Table" at line 546.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 552 
  "MPI_Abort" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 566 
  "MPI_Test" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 569 
  "HPCC_GetUpdates" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 578 
  A loop was not vectorized because it contains a call to function "MPI_Isend" on line 581.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 586 
  A loop was not vectorized because it contains a call to function "MPI_Waitany" on line 588.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 588 
  "MPI_Waitany" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 593 
  "MPI_Get_count" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6005 CC: SCALAR File = MPIRandomAccess.c, Line = 599 
  A loop was unrolled 4 times.

CC-6291 CC: VECTOR File = MPIRandomAccess.c, Line = 599 
  A loop was not vectorized because a recurrence was found between "LocalRecvBuffer" at line 600 and "HPCC_Table" at line 601.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 608 
  "MPI_Abort" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 623 
  "HPCC_FreeBuckets" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-6287 CC: VECTOR File = MPIRandomAccess.c, Line = 625 
  A loop was not vectorized because it contains a call to function "MPI_Cancel" on line 626.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 626 
  "MPI_Cancel" (called from "Power2NodesMPIRandomAccessUpdate") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 676 
  "MPI_Comm_size" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 677 
  "MPI_Comm_rank" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 680 
  "fopen" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-6254 CC: VECTOR File = MPIRandomAccess.c, Line = 689 
  A loop was not vectorized because a recurrence was found on "TotalMem" at line 691.

CC-6254 CC: VECTOR File = MPIRandomAccess.c, Line = 696 
  A loop was not vectorized because a recurrence was found on "i" at line 696.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 738 
  "MPI_Allreduce" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 762 
  "MPI_Allreduce" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 765 
  "HPCC_Power2NodesTime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the
  routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 767 
  "HPCC_AnyNodesTime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 771 
  "MPI_Allreduce" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-6290 CC: VECTOR File = MPIRandomAccess.c, Line = 804 
  A loop was not vectorized because a recurrence was found between "tparams" and "HPCC_Table" at line 805.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 807 
  "MPI_Barrier" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 809 
  "HPL_timer_cputime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 810 
  "MPI_Wtime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3118 CC: IPA File = MPIRandomAccess.c, Line = 813 
  "Power2NodesMPIRandomAccessUpdate" (called from "HPCC_MPIRandomAccess") was not inlined because the call site will not flatten. 
  "MPI_Cancel" is missing.

CC-3118 CC: IPA File = MPIRandomAccess.c, Line = 815 
  "AnyNodesMPIRandomAccessUpdate" (called from "HPCC_MPIRandomAccess") was not inlined because the call site will not flatten. 
  "MPI_Cancel" is missing.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 819 
  "MPI_Barrier" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 822 
  "HPL_timer_cputime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 823 
  "MPI_Wtime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 839 
  "MPI_Bcast" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 845 
  "HPL_timer_cputime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 846 
  "MPI_Wtime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 849 
  "HPCC_Power2NodesMPIRandomAccessCheck" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to
  locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 852 
  "HPCC_AnyNodesMPIRandomAccessCheck" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to
  locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 857 
  "MPI_Reduce" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 874 
  "HPL_timer_cputime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 875 
  "MPI_Wtime" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 901 
  "fclose" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = MPIRandomAccess.c, Line = 903 
  "MPI_Barrier" (called from "HPCC_MPIRandomAccess") was not inlined because the compiler was unable to locate the routine.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
